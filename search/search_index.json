{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Datahugger - Where DOI \ud83d\udc50 Data","text":"<p>Datahugger is a tool to download scientific datasets, software, and code from a large number of repositories based on their DOI (wiki) or URL. With Datahugger, you can automate the downloading of data and improve the reproducibility of your research. Datahugger provides a straightforward Python interface as well as an intuitive Command Line Interface (CLI).</p> <p> </p>"},{"location":"#alternatives","title":"Alternatives","text":"<p>Datahugger's main focus is on downloading datasets from arbitrary DOI's. As far as I know, there are no alternatives for downloading datasets from arbitrary DOI's. However, there are many libraries for downloading datasets from a repository right away. Usually they can also be used for uploading data to the repository. Nice examples are:</p> <ul> <li>https://guides.dataverse.org/en/latest/api/client-libraries.html</li> <li>https://pypi.org/project/pyzenodo3/</li> <li>https://osfclient.readthedocs.io/en/latest/</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#add-new-service","title":"Add new service","text":"<p>Support for repositories can be achieved by implementing a \"service\". The file datahugger/services.py list various services. For the new service, one needs to develop a new class, ideally inherited from the <code>BaseRepoDownloader</code> class. The class of Open Science Framework (<code>OSFDataset</code>) is a good example of a simple implementation.</p> <pre><code>from datahugger.base import DatasetDownloader\nfrom datahugger.base import DatasetResult\n\nclass OSFDataset(DatasetDownloader, DatasetResult):\n    \"\"\"Downloader for OSF repository.\"\"\"\n\n    REGEXP_ID = r\"osf\\.io\\/(.*)/\"\n\n    # the base entry point of the REST API\n    API_URL = \"https://api.osf.io/v2/registrations/\"\n\n    # the files and metadata about the dataset\n    API_URL_META = API_URL + \"{api_record_id}/files/osfstorage/?format=jsonapi\"\n    META_FILES_JSONPATH = \"data\"\n\n    # paths to file attributes\n    ATTR_FILE_LINK_JSONPATH = \"links.download\"\n    ATTR_NAME_JSONPATH = \"attributes.name\"\n    ATTR_SIZE_JSONPATH = \"attributes.size\"\n    ATTR_HASH_JSONPATH = \"attributes.extra.hashes.sha256\"\n    ATTR_HASH_TYPE_VALUE = \"sha256\"\n</code></pre> <ul> <li>The <code>API_URL</code> is the entry point for the URL. This URL serves the API.</li> <li>The <code>REGEXP_ID</code> is used to parse the URL and extract the ID. This ID is passed to the function <code>_get</code> with name <code>record_id</code>.</li> <li>Next, the metadata should be retrieved.</li> <li>For every file, download should be called.</li> </ul>"},{"location":"development/#datahugger-for-research-software","title":"Datahugger for research software","text":"<p>Scientific software rarely offers the options to import datasets from a DOI. Imagine what it would look like if you could. You can open a statistical software and you can start working on any published dataset. This is why we need persistent identifiers.</p>"},{"location":"download/","title":"Download dataset","text":"<p>The following example downloads dataset 10.5061/dryad.x3ffbg7m8 to the folder <code>data</code>.</p>"},{"location":"download/#download-dataset-from-doi","title":"Download dataset from DOI","text":"CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data\n</code></pre> <p>In some situations, you might have to quote the number the DOI (e.g. <code>datahugger \"10.5061/dryad.31zcrjdm5\" data</code>)</p> <pre><code>import datahugger\n\ndatahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\")\n</code></pre>"},{"location":"download/#download-dataset-from-url","title":"Download dataset from URL","text":"CLIPython <pre><code>datahugger https://datadryad.org/stash/dataset/doi:10.5061/dryad.x3ffbg7m8 data\n</code></pre> <p>In some situations, you might have to quote the number the DOI (e.g. <code>datahugger \"https://datadryad.org/stash/dataset/doi:10.5061/dryad.x3ffbg7m8\" data</code>)</p> <pre><code>import datahugger\n\ndatahugger.get(\"https://datadryad.org/stash/dataset/doi:10.5061/dryad.x3ffbg7m8\", \"data\")\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#q-does-datahugger-support-supplementary-material-for-papers","title":"Q: Does Datahugger support supplementary material for papers?","text":"<p>A: Currently, Datahugger does not support supplementary material for papers. The main challenge lies in the lack of standardization among academic journals regarding supplementary material. To our knowledge, there are no journals with proper APIs (Application Programming Interfaces) that enable seamless retrieval of supplementary materials. If you're aware of journals with such APIs or if you have specific needs within your research community, please reach out to us. Your feedback and insights can guide us in exploring potential solutions.</p>"},{"location":"faq/#q-how-can-i-report-a-bug-or-request-a-new-feature-for-datahugger","title":"Q: How can I report a bug or request a new feature for Datahugger?","text":"<p>A: To report bugs or request new features, please open an issue on Datahugger's GitHub repository. Providing detailed information about the issue or feature request helps us address it effectively.</p>"},{"location":"faq/#q-which-repositories-are-currently-supported-by-datahugger","title":"Q: Which repositories are currently supported by Datahugger?","text":"<p>A: Datahugger supports data retrieval from hundreds of repositories and millions of DOIs. You can find a list of supported repositories in the documentation. However, please note that not all repositories may be supported. We encourage maintainers of unsupported repositories to contribute to Datahugger.</p>"},{"location":"faq/#q-can-i-customize-datahugger-to-work-with-my-institutions-specific-repositories","title":"Q: Can I customize Datahugger to work with my institution's specific repositories?","text":"<p>A: Yes, you can extend Datahugger to support additional repositories by implementing a new service. Detailed guidance on creating custom services is available in the documentation, allowing you to tailor Datahugger to your institution's specific requirements.</p>"},{"location":"faq/#q-how-does-datahugger-handle-authentication-for-accessing-restricted-data","title":"Q: How does Datahugger handle authentication for accessing restricted data?","text":"<p>A: Currently, Datahugger does not provide support for accessing restricted data.</p>"},{"location":"faq/#q-is-datahugger-open-source-and-can-i-contribute-to-its-development","title":"Q: Is Datahugger open-source, and can I contribute to its development?","text":"<p>A: Yes, Datahugger is an open-source project, and we welcome contributions from the community. Information on how to contribute can be found on Datahugger's GitHub repository. We appreciate all contributions, whether they involve code, documentation improvements, or bug reports.</p>"},{"location":"faq/#q-can-datahugger-be-used-for-commercial-purposes-or-is-it-limited-to-academic-and-research-use","title":"Q: Can Datahugger be used for commercial purposes, or is it limited to academic and research use?","text":"<p>A: Datahugger is released under the MIT license, making it suitable for both academic and commercial use. It serves as a valuable tool for organizations and individuals seeking to automate data retrieval and enhance the reproducibility of research and data-related workflows.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-with-pip","title":"Installation with pip","text":"<p>Datahugger requires Python 3.6 or later.</p> <pre><code>pip install datahugger\n</code></pre>"},{"location":"installation/#upgrade","title":"Upgrade","text":"<pre><code>pip install --upgrade datahugger\n</code></pre>"},{"location":"installation/#extra-dependencies","title":"Extra dependencies","text":"<p>To download datasets from some repositories, you need addition requirements. When a service requires more dependencies, the installation instructions are given.</p> <p>To download all dependencies, use</p> <pre><code>pip install datahugger[all]\n</code></pre>"},{"location":"object/","title":"Datahugger result object","text":"<p>After you have downloaded the contents of a dataset, you can inspect the result of the download. Assign the returned object to a variable.</p> <p>This is Python API only.</p>"},{"location":"object/#scitree","title":"Scitree","text":"<p> Experimental</p> <p>The Datahugger result object has a method tree to print the tree. The tree is optimized for scientific use (see scitree and scisort).</p> Python <pre><code>import datahugger\n\ndh_data = datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\")\n\ndh_data.tree()\n</code></pre> <pre><code>data/\n\u251c\u2500\u2500 README_Pfaller_Robinson_2022_Global_Sea_Turtle_Epibiont_Database.txt\n\u2514\u2500\u2500 Pfaller_Robinson_2022_Global_Sea_Turtle_Epibiont_Database.csv\n\n0 directories, 2 files\nREADME Data Code\n</code></pre>"},{"location":"options/","title":"Options","text":""},{"location":"options/#skip-large-files","title":"Skip large files","text":"<p>For most repositories, it is possible to skip files that exceed a certain number of bytes. For example, you want to skip files larger than 50Mb.</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --max_file_size 50000000\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", max_file_size=50000000)\n</code></pre>"},{"location":"options/#extract-single-zip","title":"Extract single zip","text":"<p>Some services like Zenodo don't offer an option to preserve folder structures. Therefore, the content is often zipped before being uploaded to the service. In this case, Datahugger will unzip the file to the output folder by default.</p> <p>Disable auto unzip function</p> CLIPython <p>Not available at the moment</p> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", unzip=False)\n</code></pre>"},{"location":"options/#download-mode","title":"Download mode","text":"<p>By default, Datahugger skips the download of files and datasets that are already available on the local system. The options are: \"skip_if_exists\", \"force_redownload\".</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --download_mode force_redownload\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", download_mode=\"force_redownload\")\n</code></pre>"},{"location":"options/#progress","title":"Progress","text":"<p>By default, Datahugger shows the download progress. You can disable the progress indicator.</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --no-progress\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", progress=False)\n</code></pre>"},{"location":"repositories/","title":"Supported repositories","text":"<p>Datahugger offers support for more than 377 generic and specific (scientific) repositories (and more to come!).</p> <p></p> <p>We are still expanding Datahugger with support for more repositories. You can help by requesting support for a repository in the issue tracker. Pull Requests are very welcome as well.</p> <p>The following list gives an (non-exclusive) overview of repositories supported by Datahugger.</p>"},{"location":"repositories/#supported-repositories_1","title":"Supported repositories","text":""},{"location":"repositories/#single-implementations","title":"Single implementations","text":"<ul> <li>zenodo</li> <li>figshare</li> <li>GitHub</li> <li>DRYAD</li> <li>Hugging Face</li> <li>Open Science Framework (OSF)</li> <li>Mendeley Data</li> </ul>"},{"location":"repositories/#dataone-repositories","title":"DataOne repositories","text":"<p>The following repositories (non exhaustive) make use of DataOne software. DataOne software is supported by Datahugger.</p> <ul> <li>Arctic Data Center</li> <li>Knowledge Network for Biocomplexity (KNB)</li> <li>P\u00f4le National de Donn\u00e9es de Biodiversit\u00e9 Data Catalog</li> <li>California Ocean Protection Council</li> <li>portal.edirepository.org</li> <li>Gulf of Alaska Data Portal</li> <li>Partnership for Interdisciplinary Studies of Coastal Oceans</li> <li>Atmospheric Radiation Measurement (ARM)</li> <li>ScienceDB (Chinese Academy of Sciences)</li> <li>ESS-DIVE</li> <li>Hydroshare</li> <li>EarthChem</li> <li>IEDA2</li> <li>U.S. ANTARCTIC PROGRAM DATA CENTER</li> <li>International Year of the Salmon</li> <li>PANGAEA</li> <li>Rolling Deck to Repository (R2R)</li> <li>SEAD</li> </ul>"},{"location":"repositories/#dataverse-repositories","title":"DataVerse repositories","text":"<p>See https://dataverse.org/institutions and DataVerse on Re3data.org for an overview of DataVerse repositories.</p>"},{"location":"repositories_not_supported/","title":"Not supported repositories","text":""},{"location":"repositories_not_supported/#reasons-of-failure","title":"Reasons of failure","text":"<p>There are several reasons why Datahugger can't download the contents of a DOI. Besides internet and network errors, reasons for failure include (but not limited to):</p> <ul> <li>Not a valid DOI or URL</li> <li>DOI doesn't point to a data repository</li> <li>Data repository isn't in the list of supported repositories.</li> <li>DOI is no longer available in the repository.</li> </ul>"},{"location":"repositories_not_supported/#not-supported-error-message","title":"Not supported error message","text":"<p>When a repository is not supported, an error is returned (exit 1).</p> CLIPython <pre><code>datahugger https://hdl.handle.net/10622/NHJZUD data\n</code></pre> <pre><code>Error: Data protocol for https://hdl.handle.net/10622/NHJZUD not found.\n\nDo you think this is a data repository that needs to be supported?\nPlease request support in the issue tracker:\n\n  https://github.com/J535D165/datahugger/issues/new/choose\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\")\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/User/projects/datahugger/api.py\", line 263, in get\n    return _base_request(\n  File \"/Users/User/projects/datahugger/api.py\", line 205, in _base_request\n    raise ValueError(f\"Data protocol for {url} not found.\")\nValueError: Data protocol for https://hdl.handle.net/10622/NHJZUD not found.\n</code></pre>"},{"location":"repositories_not_supported/#request-support","title":"Request support","text":"<p>If a repository is not supported by Datahugger, you can open an issue in the GitHub issue tracker.</p>"},{"location":"reproducible/","title":"Work reproducible with Datahugger","text":"<p>By downloading a dataset directly from a DOI, you can improve the reproducibility of your (scientific) work. It also saves manual work and time. Because of this, sharing work is easier. In the following sections, we discuss possible ways to integrate Datahugger in your workflow.</p>"},{"location":"reproducible/#publish-a-project-with-datahugger","title":"Publish a project with datahugger","text":""},{"location":"reproducible/#find-a-dataset","title":"Find a dataset","text":"<p>First, find a dataset you want to use in your analysis. You can use dataset search engines like Google Dataset Search or look for references in scientific publications. Once you found the dataset, copy the DOI. If there is no DOI available, use the URL to the dataset.</p>"},{"location":"reproducible/#instruct-user-to-install-datahugger","title":"Instruct user to install datahugger","text":"<p>If the user doesn't have datahugger installed on their device, it is required to install datahugger. Datahugger can be added to an existing Python installation file like <code>requirements.txt</code> or via documentation <code>pip install datahugger</code>.</p>"},{"location":"reproducible/#scenario-1-standalone-project-setup","title":"Scenario 1: Standalone project setup","text":"<p>In this scenario, you create a script or piece of documentation to setup the prerequirements for your project. This likely contains the installation dependencies and the download of the data with Datahugger. The following example shows an example for a Python project.</p> <pre><code>pip install -r requirements.txt\ndatahugger 10.xxx/yyy data\n</code></pre> <p>This script sets up the required Python dependencies and downloads the dataset.</p>"},{"location":"reproducible/#scenario-2-single-workflow","title":"Scenario 2: Single workflow","text":"<p>In this scenario, the data download is part of the same script or workflow as the analysis. This is common for interactive environments like Jupyter Notebooks.</p>"},{"location":"reproducible/#tips-for-git-users","title":"Tips for git users","text":""},{"location":"reproducible/#add-download-folder-to-gitignore","title":"Add download folder to <code>.gitignore</code>","text":"<p>Are you using git for version control? Add the download folder to the <code>.gitignore</code> file. This prevents you from adding the new dataset to the history. As you can redownload the same dataset, there is no need to track the dataset (it's disposable).</p>"},{"location":"reproducible/#download-without-progress-indicators","title":"Download without progress indicators","text":"<p>A redownload of the data will likely result in different progress output as download times will vary. For some applications, like Jupyter Notebooks, this will result in changes in the output, where there are no changes in the actual results. To prevent this, you can disable the progress indicator.</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --no-progress\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", progress=False)\n</code></pre>"},{"location":"reproducible/#share-your-project","title":"Share your project","text":"<p>After publishing your project in a data repository or alike, others can start reusing it. Ideally, your downloaded data is not republished. This implies that the user of your project needs to download the assets with datahugger as well. This means that <code>datahugger</code> needs to be added to the installation dependencies.</p>"},{"location":"reproducible/#reuse-a-project-with-datahugger","title":"Reuse a project with datahugger","text":""},{"location":"reproducible/#install-datahugger","title":"Install datahugger","text":"<p>Ideally, the installation instructions of the project you want to (re)use provide installation instructions or automates the installation of dependencies. If not, please install datahugger.</p>"},{"location":"working/","title":"How Datahugger works","text":""},{"location":"working/#architecture","title":"Architecture","text":"<p>Datahugger solves both a conceptual and technical issue. The main challenge for datahugger is to connect the DOI to the machine-to-machine interface of the repository (the API). For humans, it is often clear to click the download button. However computers like to interact with the API. Unfortunately, there is no metadata describing the API entry point and protocol. Datahugger tries to solve this issue by creating this metadata or by doing an educated guess. The following flowchart provides an overview of the working of datahugger.</p> <p> </p>"},{"location":"working/#interesting-reads","title":"Interesting reads","text":"<p>The following articles and technical documents highlight relevant aspects of the DOI to data issue.</p> <ul> <li>Harvey MJ, Mason NJ, McLean A, Rzepa HS. Standards-based metadata procedures for retrieving data for display or mining utilizing persistent (data-DOI) identifiers. J Cheminform. 2015 Aug 8;7:37. doi: 10.1186/s13321-015-0081-7. PMID: 26257829; PMCID: PMC4528360.</li> <li>Sara Lafia &amp; Werner Kuhn (2018) Spatial Discovery of Linked Research Datasets and Documents at a Spatially Enabled Research Library, Journal of Map &amp; Geography Libraries, 14:1, 21-39, DOI: 10.1080/15420353.2018.1478923</li> <li>DOI Content Negotiation (Crosscite.org)</li> <li>Wass, Joe. \u201cURLs and DOIs: A Complicated Relationship.\u201d Crossref, https://www.crossref.org/blog/urls-and-dois-a-complicated-relationship/.</li> </ul>"}]}